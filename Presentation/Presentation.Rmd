---
title: "Linear Regression"
author: "Baptiste CRINIERE-BOIZET"
institute: Data Analysis Core
output: binb::metropolis
    # beamer_presentation:
    # theme: "metropolis"
header-includes:
  - \definecolor{mycolor}{HTML}{e23d18}
  - \definecolor{mycolor2}{HTML}{002e82}
  - \setbeamercolor{title}{fg=mycolor2}
  - \setbeamercolor{frametitle}{fg=white}
  - \setbeamercolor{progress bar}{fg=mycolor}
  - \setbeamercolor{block title}{fg=mycolor}
  - \setbeamercolor{normal text}{fg=mycolor2}
  - \setbeamercolor{palette primary}{fg=white, bg=mycolor2}
  - \setbeamercolor{palette secondary}{fg=mycolor, bg=mycolor}
  - \setbeamercolor{itemize item}{fg=mycolor}
  - \setbeamercolor{itemize subitem}{fg=mycolor}
  - \setbeamercolor{itemize subsubitem}{fg=mycolor}
  - \setbeamercolor{enumerate item}{fg=mycolor}
  - \setbeamercolor{enumerate subitem}{fg=mycolor}
  - \setbeamercolor{enumerate subsubitem}{fg=mycolor}
  - \setbeamercolor{footline}{fg=mycolor}
  - \titlegraphic{\vspace{7cm}\flushright\includegraphics[width=4cm]{ICM_rvb.png}}
  - \usepackage{animate}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(out.width="110%")
```

\tableofcontents[sectionstyle=show, subsectionstyle=hide]

# Theory
```{r, results='hide'}
library(tidyverse)
library(ggpubr)
library(MASS)
library(ggsci)
library(emmeans)
library(plotly)
set.seed(1234)
X <- rnorm(100, mean = 175, sd = 10)
Res <- rnorm(100, mean = 0, sd = 5)
Y <- X + Res
data <- data.frame(X, Y)
mod <- lm(Y ~ X, data)
summary(mod)
```


## Dependent Variable and Independent Variables



- Key Distinction: Dependent Variable (Y) vs Independent Variables (X1, X2, etc.)
\vspace{0.5cm}
- Dependent Variable = Response or Explained Variable
\vspace{0.5cm}
- Independent Variable = Predictor or Explanatory
\vspace{0.5cm}
- Choice based on logic or theory, for example: child's height depending on the mother's height



## Linear Equation

::: columns
:::: column
\begin{itemize}
\item $Y = b + a \times X$
\vspace{0.5cm}
\item $b$ : Intercept, \\ $a$ : Slope
\vspace{0.5cm}    
\item \textbf{Utility:} Allows determining $Y$ when the value of $X$ is known
\end{itemize}
::::

:::: column
```{r}
data %>% 
  ggplot(aes(x = X, y = Y))+
  geom_smooth(method = "lm", se = FALSE, color = "#e23d18", size = 4)+
  theme_classic()+
  scale_x_continuous(limits = c(145,205), n.breaks = 10)+
  scale_y_continuous(limits = c(145,205), n.breaks = 10)+
  theme(axis.line = element_line(size = 3),
        axis.title = element_text(size = 25),
        axis.text = element_blank(),
        panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent")
  )
```
::::

:::

## Linear Modeling
::: columns
:::: column
\begin{itemize}
\item Perfect Linear Relationships Rare in Real Data (NOISE)
\item \textbf{Objective}: Plot a line or predictive model capturing the observed trend
\end{itemize}
::::
:::: column
```{r}
data %>% 
  ggplot(aes(x = X, y = Y))+
  geom_point(size = 5, color = "#002e82")+
  geom_smooth(method = "lm", se = FALSE, color = "#e23d18", size = 4)+
  theme_classic()+
  scale_x_continuous(limits = c(145,205), n.breaks = 10)+
  scale_y_continuous(limits = c(145,205), n.breaks = 10)+
  theme(axis.line = element_line(size = 3),
        axis.title = element_text(size = 25),
        axis.text = element_blank(),
        panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent"))
```
::::
:::

## Residuals
::: columns
:::: column
\begin{itemize}
\item Linear Regression: Fitting a line to data with two variables
\item Limited Accuracy: Inaccuracies between observed and predicted values
\item \textbf{Residuals :} Differences between actual and predicted values
\item $e_i= y_i - \hat{y}_i$
\end{itemize}
::::
:::: column
```{r}
data$fitted3 <- 3.9 + 0.97739 * data$X

data %>% 
  ggplot(aes(x = X, y = Y))+
  geom_point(size = 5, color = "#002e82")+
  geom_smooth(method = "lm", se = FALSE, color = "#e23d18", size = 4)+
#  geom_abline(intercept = 3.9, slope = 0.97739, color = "darkred", size = 1.5)+
  geom_segment(aes(xend = X, yend = fitted3), alpha = 1, size = 2, color = "#002e82")+
  theme_classic()+
  scale_x_continuous(limits = c(145,205), n.breaks = 10)+
  scale_y_continuous(limits = c(145,205), n.breaks = 10)+
  theme(axis.line = element_line(size = 3),
        axis.title = element_text(size = 25),
        axis.text = element_blank(),
        panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent"))
```
::::
:::


## The best-fit line
::: columns

:::: column
\begin{itemize}
\item Ensure balanced prediction errors (negative and positive)
\item Residual distribution centered around 0
\item Position the line as close as possible to the data points
\item In practice, minimize the sum of squared residuals
\end{itemize}
::::

:::: column
\animategraphics[controls=none,width=\linewidth]{40}{GIF1/regression_animation-}{0}{190}

\textbf{Criteria} Lowest sum of squared residuals, representing the least squares estimator

::::
:::
# Hypothesis Checking
```{r}
set.seed(123)

age_ranges <- list(10:14, 12:16, 14:18)
ages <- unlist(lapply(age_ranges, function(x) sample(x, 20, replace = TRUE)))
noise <- rnorm(60, 0, 2)

heights1 <- 114 + 3 * ages[1:20] + noise[1:20]
heights2 <- 124 + 3 * ages[21:40] + noise[21:40]
heights3 <- 134 + 3 * ages[41:60] + noise[41:60]
heights <- c(heights1, heights2, heights3)

data <- data.frame(Age = ages, Family = as.factor(rep(1:3, each = 20)), Height = heights)
```


## List of Key Hypothesis

- \textbf{Independence}: The observations must be independent of each other
\vspace{0.5cm}
- \textbf{Linearity}: The relationship between variables must be linear
\vspace{0.5cm}
- \textbf{Homoscedasticity}: The residuals should exhibit constant variance across all predicted values
\vspace{0.5cm}
-  \textbf{Normality}: The distribution of residuals should follow a normal distribution


## Independence
::: columns
:::: column
\begin{itemize}
\item Ensure that observations are not related to each other
\item \textbf{Consequences}: Estimation bias, reliability of tests
\item \textbf{Check} : no regrouping in the distribution of residues
\end{itemize}
:::: 
:::: column
```{r}
data %>% 
  ggplot(aes(x = Age, y = Height))+
  geom_point(aes(color = Family), size = 5)+
  geom_smooth(aes(group = Family, color = Family), se = FALSE, method = "lm")+
  geom_smooth(method = "lm", se = FALSE, color = "#e23d18", size = 3, linetype = "dashed")+
  scale_color_npg()+
  theme_classic()+
  labs(x = "Age", y = "Taille")+
  theme(axis.line = element_line(size = 3),
        axis.title = element_text(size = 25),
        axis.text = element_blank(),
        panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent"),
        legend.background = element_rect(fill = "transparent"),
        legend.key = element_rect(fill = "transparent"))
```
::::
:::


## Linearity
::: columns
:::: column
\begin{itemize}
\item Concerns the relationship between variables and coefficients
\item The linear model can be adjusted with transformations (quadratic, cubic)"
\item \textbf{Consequences} : Risk of biased estimates, inaccurate predictions
\end{itemize}
:::: 
:::: column
```{r}
set.seed(123)
Age <- runif(200, 10, 18)
Res <- rnorm(200, 0, 2)
Height <- -22.5 + 22.5*Age + -0.625*Age**2 + Res
data <- data.frame(Age, Height)
curve <- function(x){
  return(-22.5 + 22.5*x + -0.625*x**2)
}

data %>% 
  ggplot(aes(x = Age, y = Height))+
  geom_point(color = "#002e82", size = 5)+
  geom_smooth(method = "lm", se = FALSE, color = "#e23d18", size = 3)+
  stat_function(fun = curve, color = "#e23d18", size = 3)+
  theme_classic()+
  labs(y = "Taille")+
  theme(axis.line = element_line(size = 3),
        axis.title = element_text(size = 25),
        axis.text = element_blank(),
        panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent"))
```
::::
:::
## Homoscedasticity
::: columns
:::: column
\begin{itemize}
\item Variability of residuals remains constant regardless of the value of the independent variable
\item \textbf{Check} : Funnel-shaped pattern or increasing/decreasing trend in the dispersion of residuals
\item \textbf{Consequences} : Risk of bias in estimations and statistical tests
\item \textbf{Solution} : Data transformation (log)
\end{itemize}
::::

:::: column
```{r}
Age <- sort(runif(100, 20, 80))
Res <- unlist(lapply(seq(0.25, 1, 0.25), function(X) rnorm(25, 0, X)))
Tps <- 0.5 + 0.05*Age + Res

data <- data.frame(Age, Tps)

data %>% 
  ggplot(aes(x = Age, y = Tps))+
  geom_point(color = "#002e82", size = 5)+
  geom_smooth(method = "lm", se = FALSE, color = "#e23d18", size = 5)+
  theme_classic()+
  labs(x = "Age",
       y = "Temps de réaction")+
  theme(axis.line = element_line(size = 3),
        axis.title = element_text(size = 25),
        axis.text = element_blank(),
        panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent"))


```

::::
:::

# Interpretation
```{r results='hide'}
set.seed(1234)
# Simulation des données
n <- 100
## Sexe
Homme <- rep(1, n)
Femme <- rep(0, n)
Sexe <- c(Homme, Femme)
## Taille
tailleH <- rnorm(n, mean = 178, sd = 8)
tailleF <- rnorm(n, mean = 164, sd = 6)
Taille <- c(tailleH, tailleF)
## Age
AgeH <- runif(n, min = 0, max = 78)
AgeF <- runif(n, min = 0, max = 84)
Age <- c(AgeH, AgeF)
## Bruit
E <- rnorm(2*n, mean = 0, sd = 7)
## Poids
Poids <- 0.4*Taille + 1.5*Sexe + 0.05*Taille*Sexe + 0.05*Age + E
data <- data.frame(Poids, Taille, Sexe, Age) %>% 
  dplyr::mutate(Sexe = Sexe %>% factor %>% 
                  forcats::fct_recode("Homme" = "1", "Femme" = "0")) 
```

## Imple regression
- \textbf{Definition} : A simple linear regression uses a single explanatory variable
\vspace{0.5cm}
- Types of explanatory variables: Continuous or Categorical
\vspace{0.5cm}
- Example: Analysis of the relationship between weight and height & weight and sex

## Régression simple
### Prédicteur continu
\vspace{1cm}
::: columns
:::: column
\begin{itemize}
\item \textbf{Modèle} : $Poids = \beta_0 + \beta_1Taille + \epsilon$
\item $\beta_0 =  -63$ et $\beta_1 = 0.81$, on teste aussi la non-nullité de chaque coefficient 
\item $R^{2}$ la part de variance expliquée par notre modèle est de 46\%
\end{itemize}
::::
:::: column
```{r echo=FALSE, fig.align='center'}
data %>% 
  ggplot(aes(x = Taille, y = Poids))+
  geom_point(size = 5, color = "#002e82")+
  geom_smooth(method = "lm", se = FALSE, color = "#e23d18", size = 5)+
  theme_classic()+
  geom_segment(aes(x = 170, y = 75, xend = 180, yend = 75), color = "red", linetype = "dashed", size = 2)+
  geom_segment(aes(x = 180, y = 75, xend = 180, yend = 83), color = "red", linetype = "dashed", size = 2)+
  theme(axis.line = element_line(size = 3),
        axis.title = element_text(size = 25),
        axis.text = element_text(size = 20),
        panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent"))
```
::::
:::

## Régression simple
### Prédicteur discret
\vspace{1cm}
::: columns
:::: column
\begin{itemize}
\item \textbf{Modèle} : $Poids = \beta_0 + \beta_1Taille + \epsilon$
\item $\beta_0 =  -63$ et $\beta_1 = 0.81$, on teste aussi la non-nullité de chaque coefficient 
\item $R^{2}$ la part de variance expliquée par notre modèle est de 46\%
\end{itemize}
::::
:::: column
```{r echo=FALSE, fig.align='center'}
data %>% 
  ggplot(aes(x = Sexe, y = Poids, color = Sexe, fill = Sexe))+
  geom_boxplot(aes(group = Sexe), alpha = 0.75, outlier.shape = NA)+
  geom_jitter(alpha = 1, shape = 1, size = 3)+
  scale_fill_manual(values = c("#002e82", "#e23d18"))+
  scale_color_manual(values = c("#002e82", "#e23d18"))+
  theme_classic()+
  theme(axis.line = element_line(size = 3),
        axis.title = element_text(size = 25),
        axis.text = element_text(size = 25),
        legend.position = "none",
        panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent"))+
  labs(x = "")
```
::::
:::

## Régression multiple

- \textbf{Définition} : régression avec plusieurs variables explicatives
\vspace{0.5cm}
- \textbf{Ajuster} notre modèle par une nouvelle variable explicative
\vspace{0.5cm}
- \textbf{Interaction} lorsque deux variables influencent simultanément la variable à expliquer

## Régression multiple : ajustement
### Cas discret
\vspace{1cm}
::: columns
:::: column
\begin{itemize}
\item \textbf{Modèle} : $Poids = \beta_0 + \beta_1 \times \text{1}_{\{Homme\}} +  \beta_2 \times Taille + \epsilon$
\item Droites parallèles représentant chaque sous-groupe d'une catégorie
\item $R^{2}$ la part de variance expliquée par notre modèle est de 55\%
\end{itemize}
::::

:::: column
```{r echo = FALSE}
data %>% 
  ggplot(aes(x = Taille, y = Poids, color = Sexe))+
  geom_point(size = 5)+
  geom_abline(intercept = -15.54, slope = 0.51, color = "#002e82", size = 4)+
  geom_abline(intercept = -15.54 + 8.85, slope = 0.51, color = "#e23d18", size = 4)+
  theme_classic()+
  scale_color_manual(values = c("#002e82",  "#e23d18"))+
  theme(axis.line = element_line(size = 3),
        axis.title = element_text(size = 25),
        axis.text = element_text(size = 25),
        legend.position = "none",
        panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent"))
```
::::
:::

## Régression multiple : ajustement
### Cas continu
\vspace{1cm}
::: columns
:::: column
\begin{itemize}
\item \textbf{Modèle} : $Poids = \beta_0 + \beta_1 \times \text{1}_{\{Homme\}} +  \beta_2 \times Taille + \epsilon$
\item Droites parallèles représentant chaque sous-groupe d'une catégorie
\item $R^{2}$ la part de variance expliquée par notre modèle est de 55\%
\end{itemize}
::::

:::: column
```{r echo = FALSE}
model <- lm(Poids ~ Taille + Age, data = data)
# summary(model)
data1 <- cbind(data,predict(model))
names(data1)[5] <- "predict"
data1 %>% 
  dplyr::mutate(cat_Age = cut_interval(Age, n = 50)) %>% 
  ggplot(aes(x = Taille, y = Poids, group = cat_Age))+
  geom_point(color = "#002e82", size = 5)+
  geom_line(aes(y = predict), color = "#e23d18")+
  theme_classic()+
  theme(axis.line = element_line(size = 3),
        axis.title = element_text(size = 25),
        axis.text = element_text(size = 25),
        legend.position = "none",
        panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent"))
```
::::
:::

## Régression multiple : interaction
### Cas discret
\vspace{1cm}
::: columns
:::: column
\begin{itemize}
\item \textbf{Définition} : Une interaction survient lorsque l'effet d'une variable sur la variable réponse dépend du niveau d'une autre variable
\item \textbf{Equation} : $Poids = \beta_0 + \beta_1 \times \text{1}_{\{Homme\}} + \beta_2 \times Taille + \beta_3 \times \text{1}_{\{Homme\}} \times Taille + \epsilon$
\item Les hommes ont une pente plus prononcée que les femmes
\end{itemize}
::::

:::: column
```{r}
data %>% 
  ggplot(aes(x = Taille, y = Poids, color = Sexe, group = Sexe))+
  geom_point(alpha = 0.3)+
  geom_smooth(method = "lm", se = FALSE, alpha = 0.7, size = 4)+
  theme_classic()+
  scale_color_manual(values = c("#002e82", "#e23d18"))+
  theme(axis.line = element_line(size = 3),
        axis.title = element_text(size = 25),
        axis.text = element_text(size = 25),
        legend.position = "none",
        panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent"))
```
::::
:::
